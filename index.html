<!DOCTYPE html>
<html lang="en">
<head>
    <title>Unsup Word Disc</title>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <style>
        img {
            max-width: 100%;
            height: auto;
        }

        body {
            font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-weight: 400;
            font-size: 16px;
            line-height: 1.5;
            color:#333;
            background-color:#fff;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
/*            margin-bottom: 15px;*/
        }

        h1, h2, h3, h4, h5, h6 {
            line-height: 1.2;
/*            margin-bottom: 15px;*/
        }

        h1 {
            text-align: center;
            font-size: 40px;  /* 42px */
            font-weight: 300;
            letter-spacing: -1px;
            line-height: 1;
/*            margin-bottom: 20px;*/
        }

        h2, h3 {
            font-size: 25px;  /* 26px */
            font-weight: 400;
        }

/*      
        @media handheld, only screen and (max-width:480px) {
            body { padding: 0; }
        }
*/

        table {
            display: block;
            width: 100%;
            border-collapse: collapse;
            overflow: auto;
            text-align: center;
        }

        a{
            color: #2a7ae2;
            text-decoration: none
        }
        a:visited{
            color: #1756a9
        }
        a:hover{
            color: #333;
            text-decoration: underline
        }

        .audio_smaller {
            max-width: 210px;  /* 195px */
        }

/*        @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400,700,italic&display=swap');*/
    </style>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:300,400,700,italic" rel="stylesheet">
</head>

<body>
    <h1>Unsupervised Word Discovery: Prominence Segmentation with Clustering and ES-KMeans+</h1>

    <p style="text-align: center">
       <a>Simon Malan</a>,
        <a href="https://scholar.google.com/citations?user=zCokvy8AAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="external noopener noreferrer">Benjamin van Niekerk</a>,
        <a href="https://www.kamperh.com/" target="_blank" rel="external noopener noreferrer">Herman Kamper</a>
    </p>
    <p style="text-align: center">
        <b>Arxiv paper:</b>
        <a href="https://arxiv.org/abs/2409.14486" target="_blank"
            rel="noopener noreferrer">https://arxiv.org/abs/2409.14486</a>
<!--         [<a href="https://arxiv.org/abs/2409.14486" target="_blank" rel="external noopener noreferrer">arXiv</a>]
        &ensp;
        [<a href="github.com/???" target="_blank" rel="external noopener noreferrer">Code</a>] -->
        <br>
        <b>Code:</b> 
        <a href="https://github.com/s-malan/prom-seg-clus" target="_blank" rel="noopener noreferrer">PromSegClus</a>, 
        <a href="https://github.com/s-malan/es-kmeans-plus" target="_blank" rel="noopener noreferrer">ES-KMeans+</a>
    </p>

    <p>
        <b>Abstract:</b> 
        We look at the long-standing problem of segmenting unlabeled speech into word-like segments and clustering these into a lexicon. 
        Several previous methods use a scoring model coupled with dynamic programming to find an optimal segmentation. 
        Here we propose a much simpler strategy: we predict word boundaries using the dissimilarity between adjacent self-supervised features, 
        then we cluster the predicted segments to construct a lexicon. 
        For a fair comparison, we update the older ES-KMeans dynamic programming method with better features and boundary constraints. 
        On the five-language ZeroSpeech benchmarks, our simple approach gives similar state-of-the-art results compared to the new ES-KMeans+ method, 
        while being almost five times faster.
    </p>

    <h2>Prominence-Based Word Segmentation</h2>

    <p>
        First we looks at the word segmentation step used in both Prom. Seg. Clus. and ES-KMeans+.
        The method is first introduced by Pasad et al. in the paper at <a href="https://arxiv.org/abs/2307.00162" target="_blank"
        rel="noopener noreferrer">https://arxiv.org/abs/2307.00162</a> titled: What do self-supervised speech models know about words? 
    </p>

    <figure>
        <img src="/prom-seg-clus/assets/promseg.png" alt="PromWordSeg"/>
        <figcaption>Fig.1: An example of word boundaries from the prominence-
            based approach of Pasad et al. The red line is the dissimilarity
            curve between adjacent frames, which is smoothed to produce
            the white line. The crosses are the predicted boundaries. The
            black vertical lines are the ground truth boundaries.</figcaption>
    </figure>

    <p>
        Figure 1 shows an example of the prominence based approach.
        The code for this step can be found in our GitHub repository: <a href="https://github.com/s-malan/prom-wordseg" target="_blank"
        rel="noopener noreferrer">https://github.com/s-malan/prom-wordseg</a>.
        The boundaries can be tuned using the parameters (distance, window_size, prominence) described in the README.
        These boundaries serve as input to the lexicon building step described next.
    </p>

    <br>

    <h2>Building a Lexicon</h2>

    <p>
        Using the prominence-based word boundaries, we can build a lexicon by simply clustering these boundaries (like in Prom. Seg. Clus.)
        or we can use dynamic programming to sub-select boundaries which are then used to build the lexicon (like in ES-KMeans+).
    </p>

    <figure>
       <img src="/prom-seg-clus/assets/lexicon.png" alt="LexiconBuild" />
        <figcaption>Fig.2: Our lexicon building step. After extracting frame-level
            features (a), PCA dimensionality reduction is applied (b). For
            each segment from the prominence-based approach (Fig. 1),
            an averaged embedding is obtained (c). These are K-means
            clustered (d) to get a lexicon.</figcaption>
    </figure>

    <p>
        Figure 2 shows the lexicon building step.
        Both Prom. Seg. Clus. and ES-KMeans+ use this step. Prom. Seg. Clus. directly builds a lexicon like in Figure 2 using the 
        prominence-based boundaries of Figure 1, the full codebase can be found in our GitHub repository: 
        <a href="https://github.com/s-malan/prom-seg-clus" target="_blank" rel="noopener noreferrer">https://github.com/s-malan/prom-seg-clus</a>.
        ES-KMeans+ also builds a lexicon like in Figure 2 but, after extracting prominence-based word boundaries like in Figure 1,
        it iteratively clusters and re-segments the utterance to select a near-optimal subset of word-like boundaries with its corresponding lexicon.
        The full codebase for ES-KMeans+ can be found in our GitHub repository: <a href="https://github.com/s-malan/es-kmeans-plus" target="_blank"
        rel="noopener noreferrer">https://github.com/s-malan/es-kmeans-plus</a>.
    </p>

    <br>

    <h2>Evaluation</h2>

    <p>
        When using Track 2 of the ZeroSpeech Challenge's datasets downloaded from <a href="https://download.zerospeech.com" target="_blank"
        rel="noopener noreferrer">https://download.zerospeech.com</a>, the ZeroSpeech toolkit <a href="https://github.com/zerospeech/benchmarks" target="_blank"
        rel="noopener noreferrer">https://github.com/zerospeech/benchmarks</a> can be used for evaluation.
        When working with <a href="https://www.openslr.org/12" target="_blank" rel="noopener noreferrer">LibriSpeech</a> or 
        <a href="https://buckeyecorpus.osu.edu/" target="_blank" rel="noopener noreferrer">BuckEye</a>, our evaluation scripts can be used, found in our
        GitHub repository: <a href="https://github.com/s-malan/evaluation" target="_blank" rel="noopener noreferrer">https://github.com/s-malan/evaluation</a>.
    </p>

    <figure>
       <img src="/prom-seg-clus/assets/results.png" alt="Results" />
        <figcaption>Table I: Performance (%) of prominence segmentation with clustering on English HuBERT, and other state-of-the-art
        methods for word segmentation and lexicon building on Track 2 of the ZeroSpeech Challenge.</figcaption>
    </figure>

    <p>
        Table I shows the results we get on the five-language ZeroSpeech Challenge.
        For more results and their discussions please refer to our paper. 
    </p>
    
    <br>

    <h2>Resynthesis</h2>

    <p>
        Resynthesis is not covered in the paper, but it provides insights into how intelligible the systems are.
        The basic premise is to recreate target (hold-out) utterances by using the learned lexicon to 
        replace word-like segments in the original utterances with segments out of the same lexicon cluster.
        This gives an audible representation of how well our systems have "learned" the language.
        We provide one example with a target utterance and its resynthesized versions.
    </p>

    <table>
        <thead>
            <tr>
                <th colspan="3" style="text-align: center;">Resynthesis Example</th>
            </tr>
            <tr>
                <th>Original/Target</th>
                <th>Prom. Seg. Clus.</th>
                <th>ES-KMeans+</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td style="text-align: center;">
                    <audio class="audio_smaller" controls controlsList="nodownload noplaybackrate" preload="metadata" src="/prom-seg-clus/assets/target.wav"></audio>
                </td>
                <td style="text-align: center;">
                    <audio class="audio_smaller" controls controlsList="nodownload noplaybackrate" preload="metadata" src="/prom-seg-clus/assets/psc_resynth.wav"></audio>
                </td>
                <td style="text-align: center;">
                    <audio class="audio_smaller" controls controlsList="nodownload noplaybackrate" preload="metadata" src="/prom-seg-clus/assets/eskm_resynth.wav"></audio>
                </td>
            </tr>
        </tbody>
    </table>
    
    <br>

</body>

</html>
